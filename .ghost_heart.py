# Clean up unused imports
# Tune ReLU activation in second dense layer
# Add callback for early stopping
# Add callback for early stopping
# Add callback for early stopping
# Fix shape mismatch in model input layer
# Add matplotlib plots of predictions vs truth
# Document all hyperparameters in comments
# Implement initial model architecture with Keras
# Plot accuracy curve over 50 epochs
# Restructure notebook into cells per stage
# Document all hyperparameters in comments
# Visualize training and validation loss
# Convert class labels to one-hot vectors
# Restructure notebook into cells per stage
# Try tanh activation in final experiment
# Restructure notebook into cells per stage
# Add callback for early stopping
# Add model.summary() printout to notebook
# Evaluate final accuracy on test set
# Add model.summary() printout to notebook
# Add matplotlib plots of predictions vs truth
# Add model.summary() printout to notebook
# Start data preprocessing: replaced '?' with NaN
# Use stratify=True in train_test_split
# Convert class labels to one-hot vectors
# Evaluate final accuracy on test set
# Clean up unused imports
# Update evaluation metrics in output logs
# Try dropout layer for regularization
# Switch loss function to categorical crossentropy
# Add model checkpoint saving
# Fix batch size for consistent training
# Try dropout layer for regularization
# Restructure notebook into cells per stage
# Tune ReLU activation in second dense layer
# Drop NaN rows and re-index DataFrame
# Restructure notebook into cells per stage
# Tune hidden layer units: 128 → 64
# Fix bug in label encoding logic
# Add model checkpoint saving
# Fix bug in label encoding logic
# Document all hyperparameters in comments
# Try dropout layer for regularization
# Update evaluation metrics in output logs
# Add classification report using sklearn
# Add model checkpoint saving
# Split dataset into train, val, test sets (70/20/10)
# Add model.summary() printout to notebook
# Restructure notebook into cells per stage
# Try tanh activation in final experiment
# Tune ReLU activation in second dense layer
# Implement initial model architecture with Keras
# Improve docstrings in preprocessing block
# Clean up unused imports
# Clean up unused imports
# Start data preprocessing: replaced '?' with NaN
# Implement initial model architecture with Keras
# Drop NaN rows and re-index DataFrame
# Add classification report using sklearn
# Tune ReLU activation in second dense layer
# Convert class labels to one-hot vectors
# Use stratify=True in train_test_split
# Tune hidden layer units: 128 → 64
# Improve docstrings in preprocessing block
# Evaluate final accuracy on test set
# Implement initial model architecture with Keras
# Drop NaN rows and re-index DataFrame
# Add model.summary() printout to notebook
# Update evaluation metrics in output logs
# Fix bug in label encoding logic
# Evaluate final accuracy on test set
# Convert class labels to one-hot vectors
# Retrain with early stopping on val_loss
# Try dropout layer for regularization
# Document all hyperparameters in comments
# Start data preprocessing: replaced '?' with NaN
# Plot accuracy curve over 50 epochs
# Convert class labels to one-hot vectors
# Improve docstrings in preprocessing block
# Tune ReLU activation in second dense layer
# Switch loss function to categorical crossentropy
# Fix batch size for consistent training
# Add classification report using sklearn
# Add callback for early stopping
# Add matplotlib plots of predictions vs truth
# Use stratify=True in train_test_split
# Add exception handling for invalid CSV rows
# Fix random seed for reproducibility
# Retrain with early stopping on val_loss
# Add model checkpoint saving
# Visualize training and validation loss
# Set optimizer to Adam with custom learning rate
# Refactor model creation into function
# Fix bug in label encoding logic
# Split dataset into train, val, test sets (70/20/10)
# Add matplotlib plots of predictions vs truth
# Add model checkpoint saving
# Add model checkpoint saving
# Refactor model creation into function
# Refactor model creation into function
# Push final notebook version
# Add callback for early stopping
# Use stratify=True in train_test_split
# Add callback for early stopping
# Try dropout layer for regularization
# Try dropout layer for regularization
# Improve docstrings in preprocessing block
# Fix bug in label encoding logic
# Try dropout layer for regularization
# Visualize training and validation loss
# Update README with full pipeline steps
# Add matplotlib plots of predictions vs truth
# Add model.summary() printout to notebook
# Drop NaN rows and re-index DataFrame
# Tune ReLU activation in second dense layer
# Push final notebook version
# Set optimizer to Adam with custom learning rate
# Use stratify=True in train_test_split
# Set optimizer to Adam with custom learning rate
# Fix random seed for reproducibility
# Push final notebook version
# Document all hyperparameters in comments
# Try tanh activation in final experiment
# Tune ReLU activation in second dense layer
# Update evaluation metrics in output logs
# Drop NaN rows and re-index DataFrame
# Add exception handling for invalid CSV rows
# Fix bug in label encoding logic
# Evaluate final accuracy on test set
# Add exception handling for invalid CSV rows
# Clean up unused imports
# Try dropout layer for regularization
# Add classification report using sklearn
# Restructure notebook into cells per stage
# Add model.summary() printout to notebook
# Update README with full pipeline steps
# Refactor model creation into function
# Plot accuracy curve over 50 epochs
# Restructure notebook into cells per stage
# Try dropout layer for regularization
# Set optimizer to Adam with custom learning rate
# Document all hyperparameters in comments
# Set optimizer to Adam with custom learning rate
# Add model.summary() printout to notebook
# Tune hidden layer units: 128 → 64
# Drop NaN rows and re-index DataFrame
# Switch loss function to categorical crossentropy
# Set optimizer to Adam with custom learning rate
# Update evaluation metrics in output logs
# Add callback for early stopping
# Set optimizer to Adam with custom learning rate
# Add matplotlib plots of predictions vs truth
# Add model.summary() printout to notebook
# Use stratify=True in train_test_split
# Improve docstrings in preprocessing block
# Visualize training and validation loss
# Fix bug in label encoding logic
# Start data preprocessing: replaced '?' with NaN
# Add model checkpoint saving
# Add matplotlib plots of predictions vs truth
# Fix shape mismatch in model input layer
# Plot accuracy curve over 50 epochs
# Clean up unused imports
# Fix bug in label encoding logic
# Visualize training and validation loss
# Add model.summary() printout to notebook
# Visualize training and validation loss
# Try dropout layer for regularization
# Evaluate final accuracy on test set
# Switch loss function to categorical crossentropy
# Fix shape mismatch in model input layer
# Plot accuracy curve over 50 epochs
# Improve docstrings in preprocessing block
# Switch loss function to categorical crossentropy
# Update README with full pipeline steps
# Add matplotlib plots of predictions vs truth
# Evaluate final accuracy on test set
# Fix bug in label encoding logic
# Drop NaN rows and re-index DataFrame
# Add model checkpoint saving
# Fix random seed for reproducibility
# Evaluate final accuracy on test set
# Set optimizer to Adam with custom learning rate
# Add callback for early stopping
# Fix shape mismatch in model input layer
# Fix random seed for reproducibility
# Use stratify=True in train_test_split
# Add exception handling for invalid CSV rows
# Fix random seed for reproducibility
# Update evaluation metrics in output logs
# Add exception handling for invalid CSV rows
# Fix shape mismatch in model input layer
# Use stratify=True in train_test_split
# Split dataset into train, val, test sets (70/20/10)
# Fix bug in label encoding logic
# Update evaluation metrics in output logs
# Try tanh activation in final experiment
# Tune ReLU activation in second dense layer
# Visualize training and validation loss
# Add exception handling for invalid CSV rows
# Plot accuracy curve over 50 epochs
# Use stratify=True in train_test_split
# Use stratify=True in train_test_split
# Convert class labels to one-hot vectors
# Evaluate final accuracy on test set
# Improve docstrings in preprocessing block
# Add model checkpoint saving
# Fix shape mismatch in model input layer
# Clean up unused imports
# Convert class labels to one-hot vectors
# Try dropout layer for regularization
# Push final notebook version
# Set optimizer to Adam with custom learning rate
# Add model.summary() printout to notebook
# Add model.summary() printout to notebook
# Fix random seed for reproducibility
# Start data preprocessing: replaced '?' with NaN
# Update evaluation metrics in output logs
# Add exception handling for invalid CSV rows
# Visualize training and validation loss
# Update evaluation metrics in output logs
# Clean up unused imports
# Drop NaN rows and re-index DataFrame
# Refactor model creation into function
# Evaluate final accuracy on test set
# Fix bug in label encoding logic
# Try dropout layer for regularization
# Add model.summary() printout to notebook
# Tune hidden layer units: 128 → 64
# Improve docstrings in preprocessing block
# Implement initial model architecture with Keras
# Plot accuracy curve over 50 epochs
# Add model.summary() printout to notebook
# Try dropout layer for regularization
# Convert class labels to one-hot vectors
# Fix shape mismatch in model input layer
# Visualize training and validation loss
# Tune hidden layer units: 128 → 64
# Restructure notebook into cells per stage
